{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flower-classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidon/pytorch_challenge/blob/master/flower_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Kp-xfEvj-LXJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ieK1K8H_wLc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Install CUDA driver\n",
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a_i3beIIEkq-",
        "colab_type": "code",
        "outputId": "de57d9b8-87d8-495e-9840-d4ff3496cf33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U pillow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pillow in /usr/local/lib/python3.6/dist-packages (5.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ex5WVc9odI3J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "from torchvision import datasets as tv_dataset, models\n",
        "from torchvision import transforms as tv_transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "colab_kernel = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "we5dTUEO5jRg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mount_gdrive():\n",
        "  from google.colab import drive\n",
        "  google_drive_mount = 'gdrive/'\n",
        "  drive.mount(\"gdrive/\", force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "835vtFEgZCbd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if colab_kernel:\n",
        "  mount_gdrive()\n",
        "  !cp 'gdrive/My Drive/colab/pytorch0/torch_helper.py' .\n",
        "  !cp 'gdrive/My Drive/colab/pytorch0/train_model.py' .\n",
        "  !cp 'gdrive/My Drive/colab/pytorch0/util_helper.py' .\n",
        "  !cp 'gdrive/My Drive/colab/pytorch0/cat_to_name.json' .\n",
        "  !cp 'gdrive/My Drive/colab/pytorch0/plt_helper.py' .\n",
        "  gpu_on = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "feQEY-2Z88gk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# My packages imports\n",
        "import torch_helper as thelper\n",
        "import plt_helper\n",
        "import util_helper\n",
        "import train_model\n",
        "\n",
        "if colab_kernel:\n",
        "  data_dir = \"gdrive/My Drive/colab/flower_data\"\n",
        "  train_dir = data_dir + '/train'\n",
        "  valid_dir = data_dir + '/valid'\n",
        "  sys.path.insert(0,'gdrive/My Drive/colab/pytorch0')\n",
        "else:\n",
        "  data_dir = \"./assets/flower_data\"\n",
        "  train_dir = os.path.join(data_dir, 'train/')\n",
        "  valid_dir = os.path.join(data_dir, 'valid/')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3vzzoFy_Qy0S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating transforms\n",
        "normalize = ([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "crop = 224\n",
        "resize = 256\n",
        "\n",
        "augment = [tv_transforms.RandomRotation(30), tv_transforms.RandomHorizontalFlip(),\n",
        "           tv_transforms.CenterCrop(224), tv_transforms.RandomVerticalFlip()]\n",
        "\n",
        "transforms = {}\n",
        "transforms['validation'] = thelper.Transforms.validation(resize=resize, crop=crop, normalize=normalize)\n",
        "transforms['train'] = thelper.Transforms.train(augment=augment, normalize=normalize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WCYBrVJ-NwQ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating datasets\n",
        "dataset = {}\n",
        "dataset['train'] = thelper.Dataset.dataset(train_dir, transforms['train'])\n",
        "dataset['validation'] = thelper.Dataset.dataset(valid_dir, transforms['validation'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t7cLBS1gOp5X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating dataloaders\n",
        "data_loader = {}\n",
        "data_loader['train'] = thelper.Loaders.loader(dataset['train'])\n",
        "data_loader['validation'] = thelper.Loaders.loader(dataset['validation'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pZAHFxDQ-_EK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Label mapping\n",
        "with open('gdrive/My Drive/colab/pytorch0/cat_to_name.json', 'r') as f:\n",
        "    cat_to_name = json.load(f)\n",
        "label_map=cat_to_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TCrig9gX4FyJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Abaixo uma celula para apresentar um batch de imagens mas no colab esta ocorrendo um erro do pill"
      ]
    },
    {
      "metadata": {
        "id": "Du9dABOIDw-J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Get a batch of training data\n",
        "# images, labels = next(iter(data_loader['train']))\n",
        "\n",
        "# # Get some images from batch\n",
        "# # images, labels = util_helper.rnd_classes(batch_images, classes_batch_images, label_map, k=4)\n",
        "\n",
        "# images = images.numpy() # convert images to numpy for display\n",
        "# labels = labels.numpy()\n",
        "\n",
        "# # plot the images in the batch, along with the corresponding labels\n",
        "# fig = plt.figure(figsize=(25, 4))\n",
        "# for idx in np.arange(20):\n",
        "#     ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "#     # plt.imshow(np.transpose(images[idx], (1, 2, 0)).astype(np.uint8))\n",
        "#     plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
        "#     ax.set_title(label_map[str(labels[idx])])\n",
        "\n",
        "\n",
        "# # # Make a grid from batch\n",
        "# # grid = make_grid(images)\n",
        "\n",
        "# # # Plot grid\n",
        "# # plt_helper.image_show(grid, title = [label[1] for label in labels])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yqu19OnlSCXQ",
        "colab_type": "code",
        "outputId": "a32aed15-5894-4a52-e3da-c18e70db80e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "# Create network\n",
        "out_features = 102\n",
        "# my_net = thelper.MyNet('resnet152', out_features=out_features)\n",
        "my_net = thelper.MyNet('vgg16', out_features=out_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at gdrive/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.torch/models/vgg16-397923af.pth\n",
            "100%|██████████| 553433881/553433881 [00:33<00:00, 16758625.57it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "itSvNsuhWErt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer (stochastic gradient descent) and learning rate = 0.001\n",
        "optimizer = my_net.create_optmizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BRsTC4sjSFYH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create training object\n",
        "train = train_model.TrainModel(my_net.model, data_loader['train'], data_loader['validation'], criterion, optimizer,   \n",
        "                               model_name=my_net.trained_model, gpu_on)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EhPzv-rySFrp",
        "colab_type": "code",
        "outputId": "79dde738-02c9-4217-c43d-6eea3c149c82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17441
        }
      },
      "cell_type": "code",
      "source": [
        "train.train()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step:  1 epoch:  0 training_loss:  4.6681437492370605\n",
            "Step:  2 epoch:  0 training_loss:  9.300402164459229\n",
            "Step:  3 epoch:  0 training_loss:  13.868295192718506\n",
            "Step:  4 epoch:  0 training_loss:  18.48255491256714\n",
            "Step:  5 epoch:  0 training_loss:  23.11095666885376\n",
            "Step:  6 epoch:  0 training_loss:  27.80910062789917\n",
            "Step:  7 epoch:  0 training_loss:  32.475882053375244\n",
            "Step:  8 epoch:  0 training_loss:  37.116429805755615\n",
            "Step:  9 epoch:  0 training_loss:  41.71677827835083\n",
            "Step:  10 epoch:  0 training_loss:  46.3550500869751\n",
            "Step:  11 epoch:  0 training_loss:  50.99484729766846\n",
            "Step:  12 epoch:  0 training_loss:  55.60764408111572\n",
            "Step:  13 epoch:  0 training_loss:  60.24363088607788\n",
            "Step:  14 epoch:  0 training_loss:  64.85625505447388\n",
            "Step:  15 epoch:  0 training_loss:  69.4524507522583\n",
            "Step:  16 epoch:  0 training_loss:  74.11461400985718\n",
            "Step:  17 epoch:  0 training_loss:  78.72568893432617\n",
            "Step:  18 epoch:  0 training_loss:  83.3608627319336\n",
            "Step:  19 epoch:  0 training_loss:  87.96230125427246\n",
            "Step:  20 epoch:  0 training_loss:  92.56041240692139\n",
            "Step:  21 epoch:  0 training_loss:  97.15858507156372\n",
            "Step:  22 epoch:  0 training_loss:  101.74192953109741\n",
            "Step:  23 epoch:  0 training_loss:  106.35711336135864\n",
            "Step:  24 epoch:  0 training_loss:  110.92960739135742\n",
            "Step:  25 epoch:  0 training_loss:  115.52828979492188\n",
            "Step:  26 epoch:  0 training_loss:  120.13645935058594\n",
            "Step:  27 epoch:  0 training_loss:  124.74159908294678\n",
            "Step:  28 epoch:  0 training_loss:  129.32095289230347\n",
            "Step:  29 epoch:  0 training_loss:  133.98057079315186\n",
            "Step:  30 epoch:  0 training_loss:  138.61499404907227\n",
            "Step:  31 epoch:  0 training_loss:  143.17286443710327\n",
            "Step:  32 epoch:  0 training_loss:  147.81638479232788\n",
            "Step:  33 epoch:  0 training_loss:  152.45360231399536\n",
            "Step:  34 epoch:  0 training_loss:  157.060791015625\n",
            "Step:  35 epoch:  0 training_loss:  161.68760538101196\n",
            "Step:  36 epoch:  0 training_loss:  166.26438331604004\n",
            "Step:  37 epoch:  0 training_loss:  170.8801145553589\n",
            "Step:  38 epoch:  0 training_loss:  175.51624822616577\n",
            "Step:  39 epoch:  0 training_loss:  180.17750597000122\n",
            "Step:  40 epoch:  0 training_loss:  184.83421087265015\n",
            "Step:  41 epoch:  0 training_loss:  189.44063520431519\n",
            "Step:  42 epoch:  0 training_loss:  194.04486894607544\n",
            "Step:  43 epoch:  0 training_loss:  198.65505409240723\n",
            "Step:  44 epoch:  0 training_loss:  203.26626682281494\n",
            "Step:  45 epoch:  0 training_loss:  207.88165712356567\n",
            "Step:  46 epoch:  0 training_loss:  212.487726688385\n",
            "Step:  47 epoch:  0 training_loss:  217.08655500411987\n",
            "Step:  48 epoch:  0 training_loss:  221.68505907058716\n",
            "Step:  49 epoch:  0 training_loss:  226.2840666770935\n",
            "Step:  50 epoch:  0 training_loss:  230.87900590896606\n",
            "Step:  51 epoch:  0 training_loss:  235.48776626586914\n",
            "Step:  52 epoch:  0 training_loss:  240.06761407852173\n",
            "Step:  53 epoch:  0 training_loss:  244.66948127746582\n",
            "Step:  54 epoch:  0 training_loss:  249.31841230392456\n",
            "Step:  55 epoch:  0 training_loss:  253.92462253570557\n",
            "Step:  56 epoch:  0 training_loss:  258.5145034790039\n",
            "Step:  57 epoch:  0 training_loss:  263.122878074646\n",
            "Step:  58 epoch:  0 training_loss:  267.7210192680359\n",
            "Step:  59 epoch:  0 training_loss:  272.2830104827881\n",
            "Step:  60 epoch:  0 training_loss:  276.8781747817993\n",
            "Step:  61 epoch:  0 training_loss:  281.5184006690979\n",
            "Step:  62 epoch:  0 training_loss:  286.09965085983276\n",
            "Step:  63 epoch:  0 training_loss:  290.6974401473999\n",
            "Step:  64 epoch:  0 training_loss:  295.3047847747803\n",
            "Step:  65 epoch:  0 training_loss:  299.89661502838135\n",
            "Step:  66 epoch:  0 training_loss:  304.52765226364136\n",
            "Step:  67 epoch:  0 training_loss:  309.1852903366089\n",
            "Step:  68 epoch:  0 training_loss:  313.7984313964844\n",
            "Step:  69 epoch:  0 training_loss:  318.3860545158386\n",
            "Step:  70 epoch:  0 training_loss:  322.994891166687\n",
            "Step:  71 epoch:  0 training_loss:  327.63524770736694\n",
            "Step:  72 epoch:  0 training_loss:  332.2386155128479\n",
            "Step:  73 epoch:  0 training_loss:  336.84747409820557\n",
            "Step:  74 epoch:  0 training_loss:  341.4335160255432\n",
            "Step:  75 epoch:  0 training_loss:  346.03763723373413\n",
            "Step:  76 epoch:  0 training_loss:  350.6646227836609\n",
            "Step:  77 epoch:  0 training_loss:  355.2457489967346\n",
            "Step:  78 epoch:  0 training_loss:  359.8820252418518\n",
            "Step:  79 epoch:  0 training_loss:  364.5007667541504\n",
            "Step:  80 epoch:  0 training_loss:  369.0980577468872\n",
            "Step:  81 epoch:  0 training_loss:  373.7590045928955\n",
            "Step:  82 epoch:  0 training_loss:  378.3342833518982\n",
            "Step:  83 epoch:  0 training_loss:  382.9148941040039\n",
            "Step:  84 epoch:  0 training_loss:  387.5026116371155\n",
            "Step:  85 epoch:  0 training_loss:  392.0616087913513\n",
            "Step:  86 epoch:  0 training_loss:  396.6071186065674\n",
            "Step:  87 epoch:  0 training_loss:  401.1709175109863\n",
            "Step:  88 epoch:  0 training_loss:  405.76210260391235\n",
            "Step:  89 epoch:  0 training_loss:  410.34671783447266\n",
            "Step:  90 epoch:  0 training_loss:  414.9707627296448\n",
            "Step:  91 epoch:  0 training_loss:  419.60724544525146\n",
            "Step:  92 epoch:  0 training_loss:  424.2110843658447\n",
            "Step:  93 epoch:  0 training_loss:  428.76090145111084\n",
            "Step:  94 epoch:  0 training_loss:  433.354718208313\n",
            "Step:  95 epoch:  0 training_loss:  438.00966119766235\n",
            "Step:  96 epoch:  0 training_loss:  442.52235746383667\n",
            "Step:  97 epoch:  0 training_loss:  447.1608319282532\n",
            "Step:  98 epoch:  0 training_loss:  451.74892950057983\n",
            "Step:  99 epoch:  0 training_loss:  456.3158130645752\n",
            "Step:  100 epoch:  0 training_loss:  460.931565284729\n",
            "Step:  101 epoch:  0 training_loss:  465.55640029907227\n",
            "Step:  102 epoch:  0 training_loss:  470.15786123275757\n",
            "Step:  103 epoch:  0 training_loss:  474.7391815185547\n",
            "Step:  104 epoch:  0 training_loss:  479.2910990715027\n",
            "Step:  105 epoch:  0 training_loss:  483.8083062171936\n",
            "Step:  106 epoch:  0 training_loss:  488.36274814605713\n",
            "Step:  107 epoch:  0 training_loss:  492.9023451805115\n",
            "Step:  108 epoch:  0 training_loss:  497.4704465866089\n",
            "Step:  109 epoch:  0 training_loss:  502.0813727378845\n",
            "Step:  110 epoch:  0 training_loss:  506.68428325653076\n",
            "Step:  111 epoch:  0 training_loss:  511.3058090209961\n",
            "Step:  112 epoch:  0 training_loss:  515.8793053627014\n",
            "Step:  113 epoch:  0 training_loss:  520.47589635849\n",
            "Step:  114 epoch:  0 training_loss:  525.1524844169617\n",
            "Step:  115 epoch:  0 training_loss:  529.7024683952332\n",
            "Step:  116 epoch:  0 training_loss:  534.2869319915771\n",
            "Step:  117 epoch:  0 training_loss:  538.8748083114624\n",
            "Step:  118 epoch:  0 training_loss:  543.4378848075867\n",
            "Step:  119 epoch:  0 training_loss:  547.9994955062866\n",
            "Step:  120 epoch:  0 training_loss:  552.5813012123108\n",
            "Step:  121 epoch:  0 training_loss:  557.192795753479\n",
            "Step:  122 epoch:  0 training_loss:  561.6843700408936\n",
            "Step:  123 epoch:  0 training_loss:  566.296510219574\n",
            "Step:  124 epoch:  0 training_loss:  570.9218797683716\n",
            "Step:  125 epoch:  0 training_loss:  575.5139484405518\n",
            "Step:  126 epoch:  0 training_loss:  580.1567525863647\n",
            "Step:  127 epoch:  0 training_loss:  584.7439289093018\n",
            "Step:  128 epoch:  0 training_loss:  589.3327107429504\n",
            "Step:  129 epoch:  0 training_loss:  593.92751121521\n",
            "Step:  130 epoch:  0 training_loss:  598.5120801925659\n",
            "Step:  131 epoch:  0 training_loss:  603.1428093910217\n",
            "Step:  132 epoch:  0 training_loss:  607.7741947174072\n",
            "Step:  133 epoch:  0 training_loss:  612.3830194473267\n",
            "Step:  134 epoch:  0 training_loss:  616.9482097625732\n",
            "Step:  135 epoch:  0 training_loss:  621.4977345466614\n",
            "Step:  136 epoch:  0 training_loss:  626.0731616020203\n",
            "Step:  137 epoch:  0 training_loss:  630.7037606239319\n",
            "Step:  138 epoch:  0 training_loss:  635.3006610870361\n",
            "Step:  139 epoch:  0 training_loss:  639.8881888389587\n",
            "Step:  140 epoch:  0 training_loss:  644.5073709487915\n",
            "Step:  141 epoch:  0 training_loss:  649.1162147521973\n",
            "Step:  142 epoch:  0 training_loss:  653.6311531066895\n",
            "Step:  143 epoch:  0 training_loss:  658.2191624641418\n",
            "Step:  144 epoch:  0 training_loss:  662.7804565429688\n",
            "Step:  145 epoch:  0 training_loss:  667.4029006958008\n",
            "Step:  146 epoch:  0 training_loss:  671.9515285491943\n",
            "Step:  147 epoch:  0 training_loss:  676.4729323387146\n",
            "Step:  148 epoch:  0 training_loss:  681.0857424736023\n",
            "Step:  149 epoch:  0 training_loss:  685.6532502174377\n",
            "Step:  150 epoch:  0 training_loss:  690.2795214653015\n",
            "Step:  151 epoch:  0 training_loss:  694.7805185317993\n",
            "Step:  152 epoch:  0 training_loss:  699.3577675819397\n",
            "Step:  153 epoch:  0 training_loss:  703.9296178817749\n",
            "Step:  154 epoch:  0 training_loss:  708.5099105834961\n",
            "Step:  155 epoch:  0 training_loss:  713.0654468536377\n",
            "Step:  156 epoch:  0 training_loss:  717.5479412078857\n",
            "Step:  157 epoch:  0 training_loss:  722.1194405555725\n",
            "Step:  158 epoch:  0 training_loss:  726.692883014679\n",
            "Step:  159 epoch:  0 training_loss:  731.2653422355652\n",
            "Step:  160 epoch:  0 training_loss:  735.8140296936035\n",
            "Step:  161 epoch:  0 training_loss:  740.3696184158325\n",
            "Step:  162 epoch:  0 training_loss:  744.9178252220154\n",
            "Step:  163 epoch:  0 training_loss:  749.49858045578\n",
            "Step:  164 epoch:  0 training_loss:  754.0387234687805\n",
            "Step:  165 epoch:  0 training_loss:  758.6405715942383\n",
            "Step:  166 epoch:  0 training_loss:  763.1828317642212\n",
            "Step:  167 epoch:  0 training_loss:  767.8409972190857\n",
            "Step:  168 epoch:  0 training_loss:  772.3731775283813\n",
            "Step:  169 epoch:  0 training_loss:  776.9094314575195\n",
            "Step:  170 epoch:  0 training_loss:  781.414158821106\n",
            "Step:  171 epoch:  0 training_loss:  786.0192470550537\n",
            "Step:  172 epoch:  0 training_loss:  790.5209264755249\n",
            "Step:  173 epoch:  0 training_loss:  795.0609970092773\n",
            "Step:  174 epoch:  0 training_loss:  799.6474833488464\n",
            "Step:  175 epoch:  0 training_loss:  804.179675579071\n",
            "Step:  176 epoch:  0 training_loss:  808.7351574897766\n",
            "Step:  177 epoch:  0 training_loss:  813.2454600334167\n",
            "Step:  178 epoch:  0 training_loss:  817.6842169761658\n",
            "Step:  179 epoch:  0 training_loss:  822.2311043739319\n",
            "Step:  180 epoch:  0 training_loss:  826.7689719200134\n",
            "Step:  181 epoch:  0 training_loss:  831.3131995201111\n",
            "Step:  182 epoch:  0 training_loss:  835.8335728645325\n",
            "Step:  183 epoch:  0 training_loss:  840.3190302848816\n",
            "Step:  184 epoch:  0 training_loss:  844.8316745758057\n",
            "Step:  185 epoch:  0 training_loss:  849.3579359054565\n",
            "Step:  186 epoch:  0 training_loss:  853.7813000679016\n",
            "Step:  187 epoch:  0 training_loss:  858.3690853118896\n",
            "Step:  188 epoch:  0 training_loss:  862.9458727836609\n",
            "Step:  189 epoch:  0 training_loss:  867.509997844696\n",
            "Step:  190 epoch:  0 training_loss:  872.0080318450928\n",
            "Step:  191 epoch:  0 training_loss:  876.5944776535034\n",
            "Step:  192 epoch:  0 training_loss:  881.1719245910645\n",
            "Step:  193 epoch:  0 training_loss:  885.6989674568176\n",
            "Step:  194 epoch:  0 training_loss:  890.2661547660828\n",
            "Step:  195 epoch:  0 training_loss:  894.8481330871582\n",
            "Step:  196 epoch:  0 training_loss:  899.3472046852112\n",
            "Step:  197 epoch:  0 training_loss:  903.8785057067871\n",
            "Step:  198 epoch:  0 training_loss:  908.4684743881226\n",
            "Step:  199 epoch:  0 training_loss:  913.0817241668701\n",
            "Step:  200 epoch:  0 training_loss:  917.5815963745117\n",
            "Step:  201 epoch:  0 training_loss:  922.1415815353394\n",
            "Step:  202 epoch:  0 training_loss:  926.7157115936279\n",
            "Step:  203 epoch:  0 training_loss:  931.3213429450989\n",
            "Step:  204 epoch:  0 training_loss:  935.8024210929871\n",
            "Step:  205 epoch:  0 training_loss:  940.3534846305847\n",
            "Step:  206 epoch:  0 training_loss:  944.8134350776672\n",
            "Step:  207 epoch:  0 training_loss:  949.3895764350891\n",
            "Step:  208 epoch:  0 training_loss:  953.9668855667114\n",
            "Step:  209 epoch:  0 training_loss:  958.4804444313049\n",
            "Step:  210 epoch:  0 training_loss:  963.017505645752\n",
            "Step:  211 epoch:  0 training_loss:  967.5453886985779\n",
            "Step:  212 epoch:  0 training_loss:  971.9970541000366\n",
            "Step:  213 epoch:  0 training_loss:  976.5082354545593\n",
            "Step:  214 epoch:  0 training_loss:  981.0817656517029\n",
            "Step:  215 epoch:  0 training_loss:  985.6700420379639\n",
            "Step:  216 epoch:  0 training_loss:  990.2203674316406\n",
            "Step:  217 epoch:  0 training_loss:  994.7305793762207\n",
            "Step:  218 epoch:  0 training_loss:  999.2240300178528\n",
            "Step:  219 epoch:  0 training_loss:  1003.779778957367\n",
            "Step:  220 epoch:  0 training_loss:  1008.2437400817871\n",
            "Step:  221 epoch:  0 training_loss:  1012.8416361808777\n",
            "Step:  222 epoch:  0 training_loss:  1017.3391790390015\n",
            "Step:  223 epoch:  0 training_loss:  1021.8858242034912\n",
            "Step:  224 epoch:  0 training_loss:  1026.4799880981445\n",
            "Step:  225 epoch:  0 training_loss:  1031.034149646759\n",
            "Step:  226 epoch:  0 training_loss:  1035.4531569480896\n",
            "Step:  227 epoch:  0 training_loss:  1040.075107574463\n",
            "Step:  228 epoch:  0 training_loss:  1044.5144262313843\n",
            "Step:  229 epoch:  0 training_loss:  1049.0776000022888\n",
            "Step:  230 epoch:  0 training_loss:  1053.6173090934753\n",
            "Step:  231 epoch:  0 training_loss:  1058.1120204925537\n",
            "Step:  232 epoch:  0 training_loss:  1062.6538171768188\n",
            "Step:  233 epoch:  0 training_loss:  1067.194396018982\n",
            "Step:  234 epoch:  0 training_loss:  1071.7026195526123\n",
            "Step:  235 epoch:  0 training_loss:  1076.2166848182678\n",
            "Step:  236 epoch:  0 training_loss:  1080.7775769233704\n",
            "Step:  237 epoch:  0 training_loss:  1085.2922224998474\n",
            "Step:  238 epoch:  0 training_loss:  1089.854040145874\n",
            "Step:  239 epoch:  0 training_loss:  1094.3422865867615\n",
            "Step:  240 epoch:  0 training_loss:  1098.8584051132202\n",
            "Step:  241 epoch:  0 training_loss:  1103.4363737106323\n",
            "Step:  242 epoch:  0 training_loss:  1107.9241948127747\n",
            "Step:  243 epoch:  0 training_loss:  1112.4492316246033\n",
            "Step:  244 epoch:  0 training_loss:  1117.0129170417786\n",
            "Step:  245 epoch:  0 training_loss:  1121.5524611473083\n",
            "Step:  246 epoch:  0 training_loss:  1126.079517841339\n",
            "Step:  247 epoch:  0 training_loss:  1130.5888857841492\n",
            "Step:  248 epoch:  0 training_loss:  1135.0940074920654\n",
            "Step:  249 epoch:  0 training_loss:  1139.4656763076782\n",
            "Step:  250 epoch:  0 training_loss:  1144.0161027908325\n",
            "Step:  251 epoch:  0 training_loss:  1148.5136523246765\n",
            "Step:  252 epoch:  0 training_loss:  1153.0792145729065\n",
            "Step:  253 epoch:  0 training_loss:  1157.649363040924\n",
            "Step:  254 epoch:  0 training_loss:  1162.0692706108093\n",
            "Step:  255 epoch:  0 training_loss:  1166.550256729126\n",
            "Step:  256 epoch:  0 training_loss:  1171.0406985282898\n",
            "Step:  257 epoch:  0 training_loss:  1175.586407661438\n",
            "Step:  258 epoch:  0 training_loss:  1180.1941938400269\n",
            "Step:  259 epoch:  0 training_loss:  1184.7708129882812\n",
            "Step:  260 epoch:  0 training_loss:  1189.2911229133606\n",
            "Step:  261 epoch:  0 training_loss:  1193.8590693473816\n",
            "Step:  262 epoch:  0 training_loss:  1198.3922872543335\n",
            "Step:  263 epoch:  0 training_loss:  1202.9521923065186\n",
            "Step:  264 epoch:  0 training_loss:  1207.427270412445\n",
            "Step:  265 epoch:  0 training_loss:  1211.972267627716\n",
            "Step:  266 epoch:  0 training_loss:  1216.4898700714111\n",
            "Step:  267 epoch:  0 training_loss:  1221.0494108200073\n",
            "Step:  268 epoch:  0 training_loss:  1225.6397647857666\n",
            "Step:  269 epoch:  0 training_loss:  1230.181134223938\n",
            "Step:  270 epoch:  0 training_loss:  1234.7576441764832\n",
            "Step:  271 epoch:  0 training_loss:  1239.2991833686829\n",
            "Step:  272 epoch:  0 training_loss:  1243.7524166107178\n",
            "Step:  273 epoch:  0 training_loss:  1248.3143863677979\n",
            "Step:  274 epoch:  0 training_loss:  1252.7561807632446\n",
            "Step:  275 epoch:  0 training_loss:  1257.3703627586365\n",
            "Step:  276 epoch:  0 training_loss:  1261.8831644058228\n",
            "Step:  277 epoch:  0 training_loss:  1266.446506023407\n",
            "Step:  278 epoch:  0 training_loss:  1270.9035334587097\n",
            "Step:  279 epoch:  0 training_loss:  1275.3234219551086\n",
            "Step:  280 epoch:  0 training_loss:  1279.7528104782104\n",
            "Step:  281 epoch:  0 training_loss:  1284.2539443969727\n",
            "Step:  282 epoch:  0 training_loss:  1288.8063440322876\n",
            "Step:  283 epoch:  0 training_loss:  1293.4432845115662\n",
            "Step:  284 epoch:  0 training_loss:  1297.8260216712952\n",
            "Step:  285 epoch:  0 training_loss:  1302.3638367652893\n",
            "Step:  286 epoch:  0 training_loss:  1306.820366859436\n",
            "Step:  287 epoch:  0 training_loss:  1311.3816828727722\n",
            "Step:  288 epoch:  0 training_loss:  1315.84987449646\n",
            "Step:  289 epoch:  0 training_loss:  1320.5062699317932\n",
            "Step:  290 epoch:  0 training_loss:  1324.988181591034\n",
            "Step:  291 epoch:  0 training_loss:  1329.428026676178\n",
            "Step:  292 epoch:  0 training_loss:  1333.9324617385864\n",
            "Step:  293 epoch:  0 training_loss:  1338.4887080192566\n",
            "Step:  294 epoch:  0 training_loss:  1343.0199813842773\n",
            "Step:  295 epoch:  0 training_loss:  1347.592206478119\n",
            "Step:  296 epoch:  0 training_loss:  1352.1298351287842\n",
            "Step:  297 epoch:  0 training_loss:  1356.6267614364624\n",
            "Step:  298 epoch:  0 training_loss:  1361.1743698120117\n",
            "Step:  299 epoch:  0 training_loss:  1365.5921392440796\n",
            "Step:  300 epoch:  0 training_loss:  1370.0663723945618\n",
            "Step:  301 epoch:  0 training_loss:  1374.6802306175232\n",
            "Step:  302 epoch:  0 training_loss:  1379.3310723304749\n",
            "Step:  303 epoch:  0 training_loss:  1383.843529701233\n",
            "Step:  304 epoch:  0 training_loss:  1388.3933901786804\n",
            "Step:  305 epoch:  0 training_loss:  1392.812138080597\n",
            "Step:  306 epoch:  0 training_loss:  1397.3282356262207\n",
            "Step:  307 epoch:  0 training_loss:  1401.8369727134705\n",
            "Step:  308 epoch:  0 training_loss:  1406.2957921028137\n",
            "Step:  309 epoch:  0 training_loss:  1410.8296117782593\n",
            "Step:  310 epoch:  0 training_loss:  1415.2713494300842\n",
            "Step:  311 epoch:  0 training_loss:  1419.784752368927\n",
            "Step:  312 epoch:  0 training_loss:  1424.261556148529\n",
            "Step:  313 epoch:  0 training_loss:  1428.7726602554321\n",
            "Step:  314 epoch:  0 training_loss:  1433.2254476547241\n",
            "Step:  315 epoch:  0 training_loss:  1437.763099193573\n",
            "Step:  316 epoch:  0 training_loss:  1442.3490982055664\n",
            "Step:  317 epoch:  0 training_loss:  1446.7815651893616\n",
            "Step:  318 epoch:  0 training_loss:  1451.345633506775\n",
            "Step:  319 epoch:  0 training_loss:  1455.8183341026306\n",
            "Step:  320 epoch:  0 training_loss:  1460.2827072143555\n",
            "Step:  321 epoch:  0 training_loss:  1464.7920503616333\n",
            "Step:  322 epoch:  0 training_loss:  1469.3446855545044\n",
            "Step:  323 epoch:  0 training_loss:  1473.7183170318604\n",
            "Step:  324 epoch:  0 training_loss:  1478.1870336532593\n",
            "Step:  325 epoch:  0 training_loss:  1482.780080795288\n",
            "Step:  326 epoch:  0 training_loss:  1487.2397212982178\n",
            "Step:  327 epoch:  0 training_loss:  1491.8006353378296\n",
            "Step:  328 epoch:  0 training_loss:  1496.2411313056946\n",
            "Epoch: 0 \tTraining Loss: 0.228364 \tValidation Loss: 4.509548\n",
            "Validation loss decreased (inf --> 4.509548).  Saving model ...\n",
            "Step:  329 epoch:  1 training_loss:  4.746305982781272\n",
            "Step:  330 epoch:  1 training_loss:  9.18241886348806\n",
            "Step:  331 epoch:  1 training_loss:  13.65339807720266\n",
            "Step:  332 epoch:  1 training_loss:  18.15508083553396\n",
            "Step:  333 epoch:  1 training_loss:  22.64573387355886\n",
            "Step:  334 epoch:  1 training_loss:  27.12899212093435\n",
            "Step:  335 epoch:  1 training_loss:  31.52384428234182\n",
            "Step:  336 epoch:  1 training_loss:  36.029896299553734\n",
            "Step:  337 epoch:  1 training_loss:  40.41869215221487\n",
            "Step:  338 epoch:  1 training_loss:  44.90398935527883\n",
            "Step:  339 epoch:  1 training_loss:  49.330348055077415\n",
            "Step:  340 epoch:  1 training_loss:  53.837396661949974\n",
            "Step:  341 epoch:  1 training_loss:  58.32212309093557\n",
            "Step:  342 epoch:  1 training_loss:  62.86762432308279\n",
            "Step:  343 epoch:  1 training_loss:  67.18135360927664\n",
            "Step:  344 epoch:  1 training_loss:  71.6388359472283\n",
            "Step:  345 epoch:  1 training_loss:  76.05834440441214\n",
            "Step:  346 epoch:  1 training_loss:  80.47761730403982\n",
            "Step:  347 epoch:  1 training_loss:  84.93420414180838\n",
            "Step:  348 epoch:  1 training_loss:  89.38495163173758\n",
            "Step:  349 epoch:  1 training_loss:  93.93365911693655\n",
            "Step:  350 epoch:  1 training_loss:  98.25982050151907\n",
            "Step:  351 epoch:  1 training_loss:  102.6385860845574\n",
            "Step:  352 epoch:  1 training_loss:  107.03404287548148\n",
            "Step:  353 epoch:  1 training_loss:  111.48010877819144\n",
            "Step:  354 epoch:  1 training_loss:  115.85823873729788\n",
            "Step:  355 epoch:  1 training_loss:  120.28150562496268\n",
            "Step:  356 epoch:  1 training_loss:  124.8084202215203\n",
            "Step:  357 epoch:  1 training_loss:  129.30454735012137\n",
            "Step:  358 epoch:  1 training_loss:  133.70891861171805\n",
            "Step:  359 epoch:  1 training_loss:  138.13932470531546\n",
            "Step:  360 epoch:  1 training_loss:  142.49039367885672\n",
            "Step:  361 epoch:  1 training_loss:  146.9333563253411\n",
            "Step:  362 epoch:  1 training_loss:  151.39476780147635\n",
            "Step:  363 epoch:  1 training_loss:  155.8938227102288\n",
            "Step:  364 epoch:  1 training_loss:  160.29471592159354\n",
            "Step:  365 epoch:  1 training_loss:  164.72743562908255\n",
            "Step:  366 epoch:  1 training_loss:  169.20840553493582\n",
            "Step:  367 epoch:  1 training_loss:  173.67820600719534\n",
            "Step:  368 epoch:  1 training_loss:  177.97295002193533\n",
            "Step:  369 epoch:  1 training_loss:  182.44641021938406\n",
            "Step:  370 epoch:  1 training_loss:  186.9674024984368\n",
            "Step:  371 epoch:  1 training_loss:  191.33525137157523\n",
            "Step:  372 epoch:  1 training_loss:  195.63398937435232\n",
            "Step:  373 epoch:  1 training_loss:  200.07022527904593\n",
            "Step:  374 epoch:  1 training_loss:  204.6215477392205\n",
            "Step:  375 epoch:  1 training_loss:  209.1190925046929\n",
            "Step:  376 epoch:  1 training_loss:  213.561553995324\n",
            "Step:  377 epoch:  1 training_loss:  218.13982681484305\n",
            "Step:  378 epoch:  1 training_loss:  222.4918518468865\n",
            "Step:  379 epoch:  1 training_loss:  226.9064984724053\n",
            "Step:  380 epoch:  1 training_loss:  231.31830648632132\n",
            "Step:  381 epoch:  1 training_loss:  235.7091255590447\n",
            "Step:  382 epoch:  1 training_loss:  239.96292976589285\n",
            "Step:  383 epoch:  1 training_loss:  244.34172682018362\n",
            "Step:  384 epoch:  1 training_loss:  248.88398651332938\n",
            "Step:  385 epoch:  1 training_loss:  253.2934556409844\n",
            "Step:  386 epoch:  1 training_loss:  257.666742365075\n",
            "Step:  387 epoch:  1 training_loss:  262.14348320217215\n",
            "Step:  388 epoch:  1 training_loss:  266.6467204496392\n",
            "Step:  389 epoch:  1 training_loss:  271.14568046779715\n",
            "Step:  390 epoch:  1 training_loss:  275.5386343404778\n",
            "Step:  391 epoch:  1 training_loss:  280.05170015544974\n",
            "Step:  392 epoch:  1 training_loss:  284.57721475811087\n",
            "Step:  393 epoch:  1 training_loss:  288.9795613691338\n",
            "Step:  394 epoch:  1 training_loss:  293.42384008617483\n",
            "Step:  395 epoch:  1 training_loss:  297.8699412748345\n",
            "Step:  396 epoch:  1 training_loss:  302.26222662181937\n",
            "Step:  397 epoch:  1 training_loss:  306.6763973638543\n",
            "Step:  398 epoch:  1 training_loss:  311.05117515773856\n",
            "Step:  399 epoch:  1 training_loss:  315.5492616102227\n",
            "Step:  400 epoch:  1 training_loss:  320.0987558767327\n",
            "Step:  401 epoch:  1 training_loss:  324.4837770864495\n",
            "Step:  402 epoch:  1 training_loss:  328.9663200780877\n",
            "Step:  403 epoch:  1 training_loss:  333.2928519651421\n",
            "Step:  404 epoch:  1 training_loss:  337.66543297023856\n",
            "Step:  405 epoch:  1 training_loss:  342.0023546621331\n",
            "Step:  406 epoch:  1 training_loss:  346.4413419172295\n",
            "Step:  407 epoch:  1 training_loss:  350.8634257719048\n",
            "Step:  408 epoch:  1 training_loss:  355.3815446302422\n",
            "Step:  409 epoch:  1 training_loss:  359.7251067564019\n",
            "Step:  410 epoch:  1 training_loss:  364.205795805169\n",
            "Step:  411 epoch:  1 training_loss:  368.7760911390313\n",
            "Step:  412 epoch:  1 training_loss:  373.24074033947073\n",
            "Step:  413 epoch:  1 training_loss:  377.75013498516165\n",
            "Step:  414 epoch:  1 training_loss:  382.0181355878838\n",
            "Step:  415 epoch:  1 training_loss:  386.30075077266775\n",
            "Step:  416 epoch:  1 training_loss:  390.6301703855523\n",
            "Step:  417 epoch:  1 training_loss:  395.093150655938\n",
            "Step:  418 epoch:  1 training_loss:  399.5283155843743\n",
            "Step:  419 epoch:  1 training_loss:  403.8978567525872\n",
            "Step:  420 epoch:  1 training_loss:  408.12243370266043\n",
            "Step:  421 epoch:  1 training_loss:  412.5912294790276\n",
            "Step:  422 epoch:  1 training_loss:  416.89421085567557\n",
            "Step:  423 epoch:  1 training_loss:  421.2355022832879\n",
            "Step:  424 epoch:  1 training_loss:  425.6936674520501\n",
            "Step:  425 epoch:  1 training_loss:  429.88760093898856\n",
            "Step:  426 epoch:  1 training_loss:  434.20397094936453\n",
            "Step:  427 epoch:  1 training_loss:  438.55354694576346\n",
            "Step:  428 epoch:  1 training_loss:  442.9399915143975\n",
            "Step:  429 epoch:  1 training_loss:  447.2171216413506\n",
            "Step:  430 epoch:  1 training_loss:  451.52838138790213\n",
            "Step:  431 epoch:  1 training_loss:  456.0078831121453\n",
            "Step:  432 epoch:  1 training_loss:  460.43604568691336\n",
            "Step:  433 epoch:  1 training_loss:  464.8390617773064\n",
            "Step:  434 epoch:  1 training_loss:  468.9539347097405\n",
            "Step:  435 epoch:  1 training_loss:  473.37881330700003\n",
            "Step:  436 epoch:  1 training_loss:  477.9479556486138\n",
            "Step:  437 epoch:  1 training_loss:  482.2000890180596\n",
            "Step:  438 epoch:  1 training_loss:  486.45994190425955\n",
            "Step:  439 epoch:  1 training_loss:  490.9219203397759\n",
            "Step:  440 epoch:  1 training_loss:  495.47954944820486\n",
            "Step:  441 epoch:  1 training_loss:  499.8059330389031\n",
            "Step:  442 epoch:  1 training_loss:  504.1889181539544\n",
            "Step:  443 epoch:  1 training_loss:  508.59319929332815\n",
            "Step:  444 epoch:  1 training_loss:  512.8983040258415\n",
            "Step:  445 epoch:  1 training_loss:  517.3697367116936\n",
            "Step:  446 epoch:  1 training_loss:  521.8045406744011\n",
            "Step:  447 epoch:  1 training_loss:  526.3319969579704\n",
            "Step:  448 epoch:  1 training_loss:  530.6253648206718\n",
            "Step:  449 epoch:  1 training_loss:  534.9237122938164\n",
            "Step:  450 epoch:  1 training_loss:  539.4387336179741\n",
            "Step:  451 epoch:  1 training_loss:  543.7315073415764\n",
            "Step:  452 epoch:  1 training_loss:  548.200663129998\n",
            "Step:  453 epoch:  1 training_loss:  552.7468920156487\n",
            "Step:  454 epoch:  1 training_loss:  557.176984350396\n",
            "Step:  455 epoch:  1 training_loss:  561.5874338552483\n",
            "Step:  456 epoch:  1 training_loss:  566.0318017408379\n",
            "Step:  457 epoch:  1 training_loss:  570.4370646879204\n",
            "Step:  458 epoch:  1 training_loss:  574.8232303068169\n",
            "Step:  459 epoch:  1 training_loss:  579.2117472097405\n",
            "Step:  460 epoch:  1 training_loss:  583.5929251119621\n",
            "Step:  461 epoch:  1 training_loss:  588.056894342614\n",
            "Step:  462 epoch:  1 training_loss:  592.5357528135307\n",
            "Step:  463 epoch:  1 training_loss:  596.9362817213066\n",
            "Step:  464 epoch:  1 training_loss:  601.1713619634636\n",
            "Step:  465 epoch:  1 training_loss:  605.6098766729363\n",
            "Step:  466 epoch:  1 training_loss:  609.77449897976\n",
            "Step:  467 epoch:  1 training_loss:  614.0694046422966\n",
            "Step:  468 epoch:  1 training_loss:  618.3753195211418\n",
            "Step:  469 epoch:  1 training_loss:  622.5918250486382\n",
            "Step:  470 epoch:  1 training_loss:  627.1279173299797\n",
            "Step:  471 epoch:  1 training_loss:  631.4684153005608\n",
            "Step:  472 epoch:  1 training_loss:  635.9893403455742\n",
            "Step:  473 epoch:  1 training_loss:  640.2760138914116\n",
            "Step:  474 epoch:  1 training_loss:  644.7110991880425\n",
            "Step:  475 epoch:  1 training_loss:  648.7867823049553\n",
            "Step:  476 epoch:  1 training_loss:  653.2732749387749\n",
            "Step:  477 epoch:  1 training_loss:  657.5002909109123\n",
            "Step:  478 epoch:  1 training_loss:  661.9279251501091\n",
            "Step:  479 epoch:  1 training_loss:  666.1133237287529\n",
            "Step:  480 epoch:  1 training_loss:  670.333207170678\n",
            "Step:  481 epoch:  1 training_loss:  674.7197170660027\n",
            "Step:  482 epoch:  1 training_loss:  679.0758657857903\n",
            "Step:  483 epoch:  1 training_loss:  683.5034318372734\n",
            "Step:  484 epoch:  1 training_loss:  687.8103905126579\n",
            "Step:  485 epoch:  1 training_loss:  691.9743071004875\n",
            "Step:  486 epoch:  1 training_loss:  696.2952638074883\n",
            "Step:  487 epoch:  1 training_loss:  700.5976577207573\n",
            "Step:  488 epoch:  1 training_loss:  705.0046697065361\n",
            "Step:  489 epoch:  1 training_loss:  709.2339239522942\n",
            "Step:  490 epoch:  1 training_loss:  713.566105405999\n",
            "Step:  491 epoch:  1 training_loss:  717.819610635949\n",
            "Step:  492 epoch:  1 training_loss:  722.0248251363762\n",
            "Step:  493 epoch:  1 training_loss:  726.5207109853752\n",
            "Step:  494 epoch:  1 training_loss:  730.8509979650505\n",
            "Step:  495 epoch:  1 training_loss:  735.3490381643303\n",
            "Step:  496 epoch:  1 training_loss:  739.593289415551\n",
            "Step:  497 epoch:  1 training_loss:  743.852123777581\n",
            "Step:  498 epoch:  1 training_loss:  748.2231822416313\n",
            "Step:  499 epoch:  1 training_loss:  752.5762219831474\n",
            "Step:  500 epoch:  1 training_loss:  756.8980803892143\n",
            "Step:  501 epoch:  1 training_loss:  761.2895589277275\n",
            "Step:  502 epoch:  1 training_loss:  765.508090536309\n",
            "Step:  503 epoch:  1 training_loss:  769.6871214315422\n",
            "Step:  504 epoch:  1 training_loss:  774.0457439825066\n",
            "Step:  505 epoch:  1 training_loss:  778.3716602727898\n",
            "Step:  506 epoch:  1 training_loss:  782.7273564741142\n",
            "Step:  507 epoch:  1 training_loss:  787.1997423574455\n",
            "Step:  508 epoch:  1 training_loss:  791.411468069268\n",
            "Step:  509 epoch:  1 training_loss:  795.853166620446\n",
            "Step:  510 epoch:  1 training_loss:  800.3017788335808\n",
            "Step:  511 epoch:  1 training_loss:  804.6178346082695\n",
            "Step:  512 epoch:  1 training_loss:  809.1776829168327\n",
            "Step:  513 epoch:  1 training_loss:  813.3493800565727\n",
            "Step:  514 epoch:  1 training_loss:  817.6154141828545\n",
            "Step:  515 epoch:  1 training_loss:  822.0358176633843\n",
            "Step:  516 epoch:  1 training_loss:  826.3786077901848\n",
            "Step:  517 epoch:  1 training_loss:  830.5877037450798\n",
            "Step:  518 epoch:  1 training_loss:  834.7617240354546\n",
            "Step:  519 epoch:  1 training_loss:  838.8245110914238\n",
            "Step:  520 epoch:  1 training_loss:  842.9954696104057\n",
            "Step:  521 epoch:  1 training_loss:  847.23304037304\n",
            "Step:  522 epoch:  1 training_loss:  851.5278835699089\n",
            "Step:  523 epoch:  1 training_loss:  855.8707495138176\n",
            "Step:  524 epoch:  1 training_loss:  860.1096096441277\n",
            "Step:  525 epoch:  1 training_loss:  864.3647394582756\n",
            "Step:  526 epoch:  1 training_loss:  868.8242302343376\n",
            "Step:  527 epoch:  1 training_loss:  873.0879187986382\n",
            "Step:  528 epoch:  1 training_loss:  877.4566789075859\n",
            "Step:  529 epoch:  1 training_loss:  881.6237869665154\n",
            "Step:  530 epoch:  1 training_loss:  885.8344307348259\n",
            "Step:  531 epoch:  1 training_loss:  890.0948152944572\n",
            "Step:  532 epoch:  1 training_loss:  894.4418163702019\n",
            "Step:  533 epoch:  1 training_loss:  898.9304428503044\n",
            "Step:  534 epoch:  1 training_loss:  903.2108331129082\n",
            "Step:  535 epoch:  1 training_loss:  907.8801770612724\n",
            "Step:  536 epoch:  1 training_loss:  912.2429523870476\n",
            "Step:  537 epoch:  1 training_loss:  916.5881076261528\n",
            "Step:  538 epoch:  1 training_loss:  920.9110222265251\n",
            "Step:  539 epoch:  1 training_loss:  925.2618875905998\n",
            "Step:  540 epoch:  1 training_loss:  929.5541887685783\n",
            "Step:  541 epoch:  1 training_loss:  934.0105548307426\n",
            "Step:  542 epoch:  1 training_loss:  938.3415012762077\n",
            "Step:  543 epoch:  1 training_loss:  942.6235933706291\n",
            "Step:  544 epoch:  1 training_loss:  946.9875717565544\n",
            "Step:  545 epoch:  1 training_loss:  951.3754702016838\n",
            "Step:  546 epoch:  1 training_loss:  955.7290025159843\n",
            "Step:  547 epoch:  1 training_loss:  959.7852960035332\n",
            "Step:  548 epoch:  1 training_loss:  963.9609637662895\n",
            "Step:  549 epoch:  1 training_loss:  968.3166604444511\n",
            "Step:  550 epoch:  1 training_loss:  972.7495780393608\n",
            "Step:  551 epoch:  1 training_loss:  977.1525936529167\n",
            "Step:  552 epoch:  1 training_loss:  981.265447656823\n",
            "Step:  553 epoch:  1 training_loss:  985.2161624834068\n",
            "Step:  554 epoch:  1 training_loss:  989.420430462075\n",
            "Step:  555 epoch:  1 training_loss:  993.5510437891014\n",
            "Step:  556 epoch:  1 training_loss:  997.7901213571556\n",
            "Step:  557 epoch:  1 training_loss:  1002.1540677949913\n",
            "Step:  558 epoch:  1 training_loss:  1006.3661792680748\n",
            "Step:  559 epoch:  1 training_loss:  1010.6877262994774\n",
            "Step:  560 epoch:  1 training_loss:  1014.8423431322105\n",
            "Step:  561 epoch:  1 training_loss:  1019.0478151247032\n",
            "Step:  562 epoch:  1 training_loss:  1023.1209896013268\n",
            "Step:  563 epoch:  1 training_loss:  1027.5078843042381\n",
            "Step:  564 epoch:  1 training_loss:  1031.6910231515892\n",
            "Step:  565 epoch:  1 training_loss:  1035.958594600869\n",
            "Step:  566 epoch:  1 training_loss:  1039.9674632951744\n",
            "Step:  567 epoch:  1 training_loss:  1044.1863611146935\n",
            "Step:  568 epoch:  1 training_loss:  1048.4844081804283\n",
            "Step:  569 epoch:  1 training_loss:  1052.8038118288048\n",
            "Step:  570 epoch:  1 training_loss:  1057.3163702890404\n",
            "Step:  571 epoch:  1 training_loss:  1061.5736096307762\n",
            "Step:  572 epoch:  1 training_loss:  1065.8473112985619\n",
            "Step:  573 epoch:  1 training_loss:  1070.0285981103905\n",
            "Step:  574 epoch:  1 training_loss:  1074.1494729921349\n",
            "Step:  575 epoch:  1 training_loss:  1078.5374091073998\n",
            "Step:  576 epoch:  1 training_loss:  1082.8571303293236\n",
            "Step:  577 epoch:  1 training_loss:  1087.1964338228233\n",
            "Step:  578 epoch:  1 training_loss:  1091.2726991579063\n",
            "Step:  579 epoch:  1 training_loss:  1095.6029370233543\n",
            "Step:  580 epoch:  1 training_loss:  1099.9406750604637\n",
            "Step:  581 epoch:  1 training_loss:  1104.1749623224266\n",
            "Step:  582 epoch:  1 training_loss:  1108.3974230691917\n",
            "Step:  583 epoch:  1 training_loss:  1112.7071850702293\n",
            "Step:  584 epoch:  1 training_loss:  1116.955345432473\n",
            "Step:  585 epoch:  1 training_loss:  1121.37300090046\n",
            "Step:  586 epoch:  1 training_loss:  1125.820425789071\n",
            "Step:  587 epoch:  1 training_loss:  1130.1026280328758\n",
            "Step:  588 epoch:  1 training_loss:  1134.3053019449242\n",
            "Step:  589 epoch:  1 training_loss:  1138.437188427163\n",
            "Step:  590 epoch:  1 training_loss:  1142.769807617379\n",
            "Step:  591 epoch:  1 training_loss:  1147.094517509652\n",
            "Step:  592 epoch:  1 training_loss:  1151.1130717203148\n",
            "Step:  593 epoch:  1 training_loss:  1155.442035000039\n",
            "Step:  594 epoch:  1 training_loss:  1159.5829213068016\n",
            "Step:  595 epoch:  1 training_loss:  1163.9869730875023\n",
            "Step:  596 epoch:  1 training_loss:  1168.2889483377464\n",
            "Step:  597 epoch:  1 training_loss:  1172.725880424691\n",
            "Step:  598 epoch:  1 training_loss:  1176.972766678048\n",
            "Step:  599 epoch:  1 training_loss:  1181.2302763864525\n",
            "Step:  600 epoch:  1 training_loss:  1185.2967937395103\n",
            "Step:  601 epoch:  1 training_loss:  1189.823382656289\n",
            "Step:  602 epoch:  1 training_loss:  1193.9733345911034\n",
            "Step:  603 epoch:  1 training_loss:  1198.2025540277489\n",
            "Step:  604 epoch:  1 training_loss:  1202.4973204538353\n",
            "Step:  605 epoch:  1 training_loss:  1206.830113689614\n",
            "Step:  606 epoch:  1 training_loss:  1211.0794585153587\n",
            "Step:  607 epoch:  1 training_loss:  1215.4448216363915\n",
            "Step:  608 epoch:  1 training_loss:  1219.5906603738792\n",
            "Step:  609 epoch:  1 training_loss:  1223.9205563470848\n",
            "Step:  610 epoch:  1 training_loss:  1228.1319077417381\n",
            "Step:  611 epoch:  1 training_loss:  1232.289543907357\n",
            "Step:  612 epoch:  1 training_loss:  1236.7323706552513\n",
            "Step:  613 epoch:  1 training_loss:  1241.049698154641\n",
            "Step:  614 epoch:  1 training_loss:  1245.3183505937584\n",
            "Step:  615 epoch:  1 training_loss:  1249.5494401857384\n",
            "Step:  616 epoch:  1 training_loss:  1253.6522663042076\n",
            "Step:  617 epoch:  1 training_loss:  1258.0830181047447\n",
            "Step:  618 epoch:  1 training_loss:  1262.1410329744347\n",
            "Step:  619 epoch:  1 training_loss:  1266.3018997118004\n",
            "Step:  620 epoch:  1 training_loss:  1270.7496750757225\n",
            "Step:  621 epoch:  1 training_loss:  1275.1525032922752\n",
            "Step:  622 epoch:  1 training_loss:  1279.4087994501122\n",
            "Step:  623 epoch:  1 training_loss:  1283.6100166246422\n",
            "Step:  624 epoch:  1 training_loss:  1287.7984669610985\n",
            "Step:  625 epoch:  1 training_loss:  1292.4023363992699\n",
            "Step:  626 epoch:  1 training_loss:  1296.8381951257713\n",
            "Step:  627 epoch:  1 training_loss:  1301.148864547921\n",
            "Step:  628 epoch:  1 training_loss:  1305.429623405648\n",
            "Step:  629 epoch:  1 training_loss:  1309.5935662195213\n",
            "Step:  630 epoch:  1 training_loss:  1313.9500792428978\n",
            "Step:  631 epoch:  1 training_loss:  1317.9396002695091\n",
            "Step:  632 epoch:  1 training_loss:  1322.2683785364159\n",
            "Step:  633 epoch:  1 training_loss:  1326.737261573983\n",
            "Step:  634 epoch:  1 training_loss:  1331.000891487313\n",
            "Step:  635 epoch:  1 training_loss:  1335.1305077478416\n",
            "Step:  636 epoch:  1 training_loss:  1339.4604971811302\n",
            "Step:  637 epoch:  1 training_loss:  1343.852993290139\n",
            "Step:  638 epoch:  1 training_loss:  1347.9072297021874\n",
            "Step:  639 epoch:  1 training_loss:  1352.2074301645287\n",
            "Step:  640 epoch:  1 training_loss:  1356.4495752260216\n",
            "Step:  641 epoch:  1 training_loss:  1360.843288223458\n",
            "Step:  642 epoch:  1 training_loss:  1365.141546051217\n",
            "Step:  643 epoch:  1 training_loss:  1369.6069190904625\n",
            "Step:  644 epoch:  1 training_loss:  1373.9240911409386\n",
            "Step:  645 epoch:  1 training_loss:  1378.1239216730125\n",
            "Step:  646 epoch:  1 training_loss:  1382.3504546091087\n",
            "Step:  647 epoch:  1 training_loss:  1386.4782360002525\n",
            "Step:  648 epoch:  1 training_loss:  1390.7248590394981\n",
            "Step:  649 epoch:  1 training_loss:  1394.90469054432\n",
            "Step:  650 epoch:  1 training_loss:  1399.174417774392\n",
            "Step:  651 epoch:  1 training_loss:  1403.4180066988\n",
            "Step:  652 epoch:  1 training_loss:  1407.8278234407433\n",
            "Step:  653 epoch:  1 training_loss:  1412.0689833566673\n",
            "Step:  654 epoch:  1 training_loss:  1416.2637088701256\n",
            "Step:  655 epoch:  1 training_loss:  1420.7680886194237\n",
            "Step:  656 epoch:  1 training_loss:  1425.0955264970787\n",
            "Epoch: 1 \tTraining Loss: 0.217505 \tValidation Loss: 4.336639\n",
            "Validation loss decreased (4.509548 --> 4.336639).  Saving model ...\n",
            "Step:  657 epoch:  2 training_loss:  4.31204601824534\n",
            "Step:  658 epoch:  2 training_loss:  8.330808129909158\n",
            "Step:  659 epoch:  2 training_loss:  12.131629911067606\n",
            "Step:  660 epoch:  2 training_loss:  16.430487599971414\n",
            "Step:  661 epoch:  2 training_loss:  20.801660028102518\n",
            "Step:  662 epoch:  2 training_loss:  24.796722617747903\n",
            "Step:  663 epoch:  2 training_loss:  28.7708232075001\n",
            "Step:  664 epoch:  2 training_loss:  32.938010421397806\n",
            "Step:  665 epoch:  2 training_loss:  37.177995887401224\n",
            "Step:  666 epoch:  2 training_loss:  41.40497132838023\n",
            "Step:  667 epoch:  2 training_loss:  45.375722613933206\n",
            "Step:  668 epoch:  2 training_loss:  49.57408067286265\n",
            "Step:  669 epoch:  2 training_loss:  53.791191783549905\n",
            "Step:  670 epoch:  2 training_loss:  57.768643108012796\n",
            "Step:  671 epoch:  2 training_loss:  62.106712070109964\n",
            "Step:  672 epoch:  2 training_loss:  66.4039494663502\n",
            "Step:  673 epoch:  2 training_loss:  70.24367424547923\n",
            "Step:  674 epoch:  2 training_loss:  74.50890108645213\n",
            "Step:  675 epoch:  2 training_loss:  78.74238440096629\n",
            "Step:  676 epoch:  2 training_loss:  82.81988951266062\n",
            "Step:  677 epoch:  2 training_loss:  86.93747564852488\n",
            "Step:  678 epoch:  2 training_loss:  91.15872522890818\n",
            "Step:  679 epoch:  2 training_loss:  95.14751144945872\n",
            "Step:  680 epoch:  2 training_loss:  99.30097385943186\n",
            "Step:  681 epoch:  2 training_loss:  103.5894689232136\n",
            "Step:  682 epoch:  2 training_loss:  107.70332333147776\n",
            "Step:  683 epoch:  2 training_loss:  111.85811659395945\n",
            "Step:  684 epoch:  2 training_loss:  116.28599258959544\n",
            "Step:  685 epoch:  2 training_loss:  120.47974201739085\n",
            "Step:  686 epoch:  2 training_loss:  124.45812269747508\n",
            "Step:  687 epoch:  2 training_loss:  128.59111925661813\n",
            "Step:  688 epoch:  2 training_loss:  132.64527699053536\n",
            "Step:  689 epoch:  2 training_loss:  136.83240219652902\n",
            "Step:  690 epoch:  2 training_loss:  140.94380899966012\n",
            "Step:  691 epoch:  2 training_loss:  144.83149906695138\n",
            "Step:  692 epoch:  2 training_loss:  148.9759592682148\n",
            "Step:  693 epoch:  2 training_loss:  152.96832677424203\n",
            "Step:  694 epoch:  2 training_loss:  157.23575755656015\n",
            "Step:  695 epoch:  2 training_loss:  161.39639207422982\n",
            "Step:  696 epoch:  2 training_loss:  165.45325919687997\n",
            "Step:  697 epoch:  2 training_loss:  169.35515400469552\n",
            "Step:  698 epoch:  2 training_loss:  173.1827978760029\n",
            "Step:  699 epoch:  2 training_loss:  177.3490585953022\n",
            "Step:  700 epoch:  2 training_loss:  181.345802751186\n",
            "Step:  701 epoch:  2 training_loss:  185.45214935839425\n",
            "Step:  702 epoch:  2 training_loss:  189.63422390520822\n",
            "Step:  703 epoch:  2 training_loss:  193.92241760790597\n",
            "Step:  704 epoch:  2 training_loss:  198.01306482851754\n",
            "Step:  705 epoch:  2 training_loss:  202.35335585177194\n",
            "Step:  706 epoch:  2 training_loss:  206.3788699775959\n",
            "Step:  707 epoch:  2 training_loss:  210.61336180269967\n",
            "Step:  708 epoch:  2 training_loss:  214.7561893135334\n",
            "Step:  709 epoch:  2 training_loss:  218.78686281740914\n",
            "Step:  710 epoch:  2 training_loss:  223.10321756899606\n",
            "Step:  711 epoch:  2 training_loss:  227.54829832613717\n",
            "Step:  712 epoch:  2 training_loss:  231.78128620684396\n",
            "Step:  713 epoch:  2 training_loss:  235.84262177050363\n",
            "Step:  714 epoch:  2 training_loss:  240.1635918289448\n",
            "Step:  715 epoch:  2 training_loss:  244.3331246048237\n",
            "Step:  716 epoch:  2 training_loss:  248.6364488273884\n",
            "Step:  717 epoch:  2 training_loss:  252.6940841347004\n",
            "Step:  718 epoch:  2 training_loss:  257.077775445583\n",
            "Step:  719 epoch:  2 training_loss:  261.33594414294015\n",
            "Step:  720 epoch:  2 training_loss:  265.44224592745553\n",
            "Step:  721 epoch:  2 training_loss:  269.5831885010029\n",
            "Step:  722 epoch:  2 training_loss:  273.63144442141305\n",
            "Step:  723 epoch:  2 training_loss:  277.845190969112\n",
            "Step:  724 epoch:  2 training_loss:  282.0227741867329\n",
            "Step:  725 epoch:  2 training_loss:  286.12993713915597\n",
            "Step:  726 epoch:  2 training_loss:  290.4876041084553\n",
            "Step:  727 epoch:  2 training_loss:  294.6051292091633\n",
            "Step:  728 epoch:  2 training_loss:  298.79847427904855\n",
            "Step:  729 epoch:  2 training_loss:  303.06489559710275\n",
            "Step:  730 epoch:  2 training_loss:  307.2847184807087\n",
            "Step:  731 epoch:  2 training_loss:  311.302448239925\n",
            "Step:  732 epoch:  2 training_loss:  315.2520184189106\n",
            "Step:  733 epoch:  2 training_loss:  319.21066567003976\n",
            "Step:  734 epoch:  2 training_loss:  323.3715471893574\n",
            "Step:  735 epoch:  2 training_loss:  327.3883380562092\n",
            "Step:  736 epoch:  2 training_loss:  331.61898752749215\n",
            "Step:  737 epoch:  2 training_loss:  335.6275205284382\n",
            "Step:  738 epoch:  2 training_loss:  339.61050888598214\n",
            "Step:  739 epoch:  2 training_loss:  343.8576926857258\n",
            "Step:  740 epoch:  2 training_loss:  348.17916246950875\n",
            "Step:  741 epoch:  2 training_loss:  352.25704094469796\n",
            "Step:  742 epoch:  2 training_loss:  356.478850331905\n",
            "Step:  743 epoch:  2 training_loss:  360.616150823238\n",
            "Step:  744 epoch:  2 training_loss:  365.08754822314035\n",
            "Step:  745 epoch:  2 training_loss:  369.44528385699044\n",
            "Step:  746 epoch:  2 training_loss:  373.58301207125436\n",
            "Step:  747 epoch:  2 training_loss:  377.70745989382516\n",
            "Step:  748 epoch:  2 training_loss:  382.04258772433053\n",
            "Step:  749 epoch:  2 training_loss:  386.1588639885212\n",
            "Step:  750 epoch:  2 training_loss:  390.120353904369\n",
            "Step:  751 epoch:  2 training_loss:  394.1249076992298\n",
            "Step:  752 epoch:  2 training_loss:  398.09896465838204\n",
            "Step:  753 epoch:  2 training_loss:  402.3212585121418\n",
            "Step:  754 epoch:  2 training_loss:  406.2555713325764\n",
            "Step:  755 epoch:  2 training_loss:  410.38130280077706\n",
            "Step:  756 epoch:  2 training_loss:  414.42477032244454\n",
            "Step:  757 epoch:  2 training_loss:  418.383653607967\n",
            "Step:  758 epoch:  2 training_loss:  422.2090792328144\n",
            "Step:  759 epoch:  2 training_loss:  425.83894821703683\n",
            "Step:  760 epoch:  2 training_loss:  430.1754040390278\n",
            "Step:  761 epoch:  2 training_loss:  434.20926376879464\n",
            "Step:  762 epoch:  2 training_loss:  438.5125031143452\n",
            "Step:  763 epoch:  2 training_loss:  442.87445780337106\n",
            "Step:  764 epoch:  2 training_loss:  446.80550405085336\n",
            "Step:  765 epoch:  2 training_loss:  451.0805527836109\n",
            "Step:  766 epoch:  2 training_loss:  455.19476481974374\n",
            "Step:  767 epoch:  2 training_loss:  459.4046580463673\n",
            "Step:  768 epoch:  2 training_loss:  463.41161986887704\n",
            "Step:  769 epoch:  2 training_loss:  467.53639384806405\n",
            "Step:  770 epoch:  2 training_loss:  471.38767024576913\n",
            "Step:  771 epoch:  2 training_loss:  475.65848037302743\n",
            "Step:  772 epoch:  2 training_loss:  479.6454524666096\n",
            "Step:  773 epoch:  2 training_loss:  483.7913789421345\n",
            "Step:  774 epoch:  2 training_loss:  487.79857822954904\n",
            "Step:  775 epoch:  2 training_loss:  491.85647245943795\n",
            "Step:  776 epoch:  2 training_loss:  496.18256804049264\n",
            "Step:  777 epoch:  2 training_loss:  499.85188528597604\n",
            "Step:  778 epoch:  2 training_loss:  503.8618669182087\n",
            "Step:  779 epoch:  2 training_loss:  507.9373693138386\n",
            "Step:  780 epoch:  2 training_loss:  512.1836838394429\n",
            "Step:  781 epoch:  2 training_loss:  516.151965346935\n",
            "Step:  782 epoch:  2 training_loss:  520.3500291973378\n",
            "Step:  783 epoch:  2 training_loss:  524.1275643974568\n",
            "Step:  784 epoch:  2 training_loss:  528.4178409248616\n",
            "Step:  785 epoch:  2 training_loss:  532.5454663902547\n",
            "Step:  786 epoch:  2 training_loss:  536.482015100124\n",
            "Step:  787 epoch:  2 training_loss:  540.4779619842793\n",
            "Step:  788 epoch:  2 training_loss:  544.5226320892598\n",
            "Step:  789 epoch:  2 training_loss:  548.5689658790852\n",
            "Step:  790 epoch:  2 training_loss:  552.8663806587483\n",
            "Step:  791 epoch:  2 training_loss:  557.034197774532\n",
            "Step:  792 epoch:  2 training_loss:  560.6280405193593\n",
            "Step:  793 epoch:  2 training_loss:  564.3842398792531\n",
            "Step:  794 epoch:  2 training_loss:  568.475786414745\n",
            "Step:  795 epoch:  2 training_loss:  572.7589523464467\n",
            "Step:  796 epoch:  2 training_loss:  576.9639765888478\n",
            "Step:  797 epoch:  2 training_loss:  581.2430441051747\n",
            "Step:  798 epoch:  2 training_loss:  585.5362407833363\n",
            "Step:  799 epoch:  2 training_loss:  589.2513756424214\n",
            "Step:  800 epoch:  2 training_loss:  593.5011706024434\n",
            "Step:  801 epoch:  2 training_loss:  597.4949927002217\n",
            "Step:  802 epoch:  2 training_loss:  601.6727909714009\n",
            "Step:  803 epoch:  2 training_loss:  605.7104124695088\n",
            "Step:  804 epoch:  2 training_loss:  609.6631071239735\n",
            "Step:  805 epoch:  2 training_loss:  613.8206836849477\n",
            "Step:  806 epoch:  2 training_loss:  618.2248603969838\n",
            "Step:  807 epoch:  2 training_loss:  622.2478925853993\n",
            "Step:  808 epoch:  2 training_loss:  626.2325021892811\n",
            "Step:  809 epoch:  2 training_loss:  630.3994943767811\n",
            "Step:  810 epoch:  2 training_loss:  634.4623462826039\n",
            "Step:  811 epoch:  2 training_loss:  638.7036335140492\n",
            "Step:  812 epoch:  2 training_loss:  642.5922696262624\n",
            "Step:  813 epoch:  2 training_loss:  646.4921986728932\n",
            "Step:  814 epoch:  2 training_loss:  650.7674629360463\n",
            "Step:  815 epoch:  2 training_loss:  655.0663110882069\n",
            "Step:  816 epoch:  2 training_loss:  659.2425262600209\n",
            "Step:  817 epoch:  2 training_loss:  663.4307615429188\n",
            "Step:  818 epoch:  2 training_loss:  667.4667126804616\n",
            "Step:  819 epoch:  2 training_loss:  671.4153978496815\n",
            "Step:  820 epoch:  2 training_loss:  675.395635810497\n",
            "Step:  821 epoch:  2 training_loss:  679.3014344841267\n",
            "Step:  822 epoch:  2 training_loss:  683.3934669166829\n",
            "Step:  823 epoch:  2 training_loss:  687.3510722786214\n",
            "Step:  824 epoch:  2 training_loss:  691.0723781257893\n",
            "Step:  825 epoch:  2 training_loss:  695.0503611236836\n",
            "Step:  826 epoch:  2 training_loss:  698.8811678558613\n",
            "Step:  827 epoch:  2 training_loss:  702.8282594352986\n",
            "Step:  828 epoch:  2 training_loss:  706.9960527092244\n",
            "Step:  829 epoch:  2 training_loss:  711.1228766113545\n",
            "Step:  830 epoch:  2 training_loss:  715.3586182266499\n",
            "Step:  831 epoch:  2 training_loss:  719.4835257202412\n",
            "Step:  832 epoch:  2 training_loss:  723.6289539009358\n",
            "Step:  833 epoch:  2 training_loss:  727.8478750854756\n",
            "Step:  834 epoch:  2 training_loss:  732.3106693893697\n",
            "Step:  835 epoch:  2 training_loss:  736.4068236023213\n",
            "Step:  836 epoch:  2 training_loss:  740.2830228477742\n",
            "Step:  837 epoch:  2 training_loss:  744.6301383644368\n",
            "Step:  838 epoch:  2 training_loss:  748.9612321525838\n",
            "Step:  839 epoch:  2 training_loss:  753.1113414436604\n",
            "Step:  840 epoch:  2 training_loss:  756.8328289657857\n",
            "Step:  841 epoch:  2 training_loss:  761.0946778923299\n",
            "Step:  842 epoch:  2 training_loss:  765.2406892448689\n",
            "Step:  843 epoch:  2 training_loss:  769.455807653072\n",
            "Step:  844 epoch:  2 training_loss:  773.4100680023457\n",
            "Step:  845 epoch:  2 training_loss:  777.5705818802144\n",
            "Step:  846 epoch:  2 training_loss:  781.5039760738637\n",
            "Step:  847 epoch:  2 training_loss:  785.3984954029347\n",
            "Step:  848 epoch:  2 training_loss:  789.3703663021352\n",
            "Step:  849 epoch:  2 training_loss:  793.6128785282399\n",
            "Step:  850 epoch:  2 training_loss:  797.5780460506703\n",
            "Step:  851 epoch:  2 training_loss:  801.563000169399\n",
            "Step:  852 epoch:  2 training_loss:  805.3258983761098\n",
            "Step:  853 epoch:  2 training_loss:  809.2941093117024\n",
            "Step:  854 epoch:  2 training_loss:  813.3781170517232\n",
            "Step:  855 epoch:  2 training_loss:  817.0766682297017\n",
            "Step:  856 epoch:  2 training_loss:  820.9143039852406\n",
            "Step:  857 epoch:  2 training_loss:  824.7736072212483\n",
            "Step:  858 epoch:  2 training_loss:  829.1030158668782\n",
            "Step:  859 epoch:  2 training_loss:  833.3874382644917\n",
            "Step:  860 epoch:  2 training_loss:  837.3376159340169\n",
            "Step:  861 epoch:  2 training_loss:  841.4332408577229\n",
            "Step:  862 epoch:  2 training_loss:  845.6656541496541\n",
            "Step:  863 epoch:  2 training_loss:  849.6785716682698\n",
            "Step:  864 epoch:  2 training_loss:  853.8205151230122\n",
            "Step:  865 epoch:  2 training_loss:  858.0383677154805\n",
            "Step:  866 epoch:  2 training_loss:  861.8617972999837\n",
            "Step:  867 epoch:  2 training_loss:  865.9816269546773\n",
            "Step:  868 epoch:  2 training_loss:  870.1559404999043\n",
            "Step:  869 epoch:  2 training_loss:  874.2777971893574\n",
            "Step:  870 epoch:  2 training_loss:  877.7265073925282\n",
            "Step:  871 epoch:  2 training_loss:  882.0316559940602\n",
            "Step:  872 epoch:  2 training_loss:  885.8705682426717\n",
            "Step:  873 epoch:  2 training_loss:  889.8023931652333\n",
            "Step:  874 epoch:  2 training_loss:  893.6102521091725\n",
            "Step:  875 epoch:  2 training_loss:  897.4641937881734\n",
            "Step:  876 epoch:  2 training_loss:  901.8255014091756\n",
            "Step:  877 epoch:  2 training_loss:  905.952259984615\n",
            "Step:  878 epoch:  2 training_loss:  910.0285591751363\n",
            "Step:  879 epoch:  2 training_loss:  913.902485337856\n",
            "Step:  880 epoch:  2 training_loss:  917.7715110451009\n",
            "Step:  881 epoch:  2 training_loss:  921.845287290218\n",
            "Step:  882 epoch:  2 training_loss:  926.0941781669881\n",
            "Step:  883 epoch:  2 training_loss:  930.4544987350728\n",
            "Step:  884 epoch:  2 training_loss:  934.2767688900258\n",
            "Step:  885 epoch:  2 training_loss:  938.4111835628773\n",
            "Step:  886 epoch:  2 training_loss:  942.1639575630452\n",
            "Step:  887 epoch:  2 training_loss:  946.3753938347127\n",
            "Step:  888 epoch:  2 training_loss:  950.4897374779011\n",
            "Step:  889 epoch:  2 training_loss:  954.4816250473286\n",
            "Step:  890 epoch:  2 training_loss:  958.4184994369771\n",
            "Step:  891 epoch:  2 training_loss:  962.4141819149281\n",
            "Step:  892 epoch:  2 training_loss:  966.2800662189748\n",
            "Step:  893 epoch:  2 training_loss:  970.5615002781178\n",
            "Step:  894 epoch:  2 training_loss:  974.8087889820363\n",
            "Step:  895 epoch:  2 training_loss:  978.9691970020558\n",
            "Step:  896 epoch:  2 training_loss:  983.1211907535817\n",
            "Step:  897 epoch:  2 training_loss:  987.0378610759999\n",
            "Step:  898 epoch:  2 training_loss:  990.988315787914\n",
            "Step:  899 epoch:  2 training_loss:  994.9431490570332\n",
            "Step:  900 epoch:  2 training_loss:  999.1129808098103\n",
            "Step:  901 epoch:  2 training_loss:  1003.1609325081135\n",
            "Step:  902 epoch:  2 training_loss:  1007.3210425049092\n",
            "Step:  903 epoch:  2 training_loss:  1011.4771022468831\n",
            "Step:  904 epoch:  2 training_loss:  1015.713939633968\n",
            "Step:  905 epoch:  2 training_loss:  1019.8682675033833\n",
            "Step:  906 epoch:  2 training_loss:  1023.8244323402669\n",
            "Step:  907 epoch:  2 training_loss:  1027.8703236252095\n",
            "Step:  908 epoch:  2 training_loss:  1032.1633586555745\n",
            "Step:  909 epoch:  2 training_loss:  1036.3641051918294\n",
            "Step:  910 epoch:  2 training_loss:  1040.2488600879933\n",
            "Step:  911 epoch:  2 training_loss:  1044.2878648906972\n",
            "Step:  912 epoch:  2 training_loss:  1048.1572758823659\n",
            "Step:  913 epoch:  2 training_loss:  1051.9027542740132\n",
            "Step:  914 epoch:  2 training_loss:  1055.8786544472005\n",
            "Step:  915 epoch:  2 training_loss:  1059.7340907722737\n",
            "Step:  916 epoch:  2 training_loss:  1063.7692665725972\n",
            "Step:  917 epoch:  2 training_loss:  1067.8044032722737\n",
            "Step:  918 epoch:  2 training_loss:  1072.0181102424885\n",
            "Step:  919 epoch:  2 training_loss:  1075.9906959205891\n",
            "Step:  920 epoch:  2 training_loss:  1079.812520948055\n",
            "Step:  921 epoch:  2 training_loss:  1084.0657357841756\n",
            "Step:  922 epoch:  2 training_loss:  1088.0848865181233\n",
            "Step:  923 epoch:  2 training_loss:  1092.1883072525288\n",
            "Step:  924 epoch:  2 training_loss:  1096.0945138603474\n",
            "Step:  925 epoch:  2 training_loss:  1099.7222847610738\n",
            "Step:  926 epoch:  2 training_loss:  1103.8373140960957\n",
            "Step:  927 epoch:  2 training_loss:  1107.5761255890156\n",
            "Step:  928 epoch:  2 training_loss:  1111.6114010483052\n",
            "Step:  929 epoch:  2 training_loss:  1115.309550967815\n",
            "Step:  930 epoch:  2 training_loss:  1119.000943151119\n",
            "Step:  931 epoch:  2 training_loss:  1122.7143151432301\n",
            "Step:  932 epoch:  2 training_loss:  1126.9459755092885\n",
            "Step:  933 epoch:  2 training_loss:  1130.8553416401173\n",
            "Step:  934 epoch:  2 training_loss:  1134.0448503166463\n",
            "Step:  935 epoch:  2 training_loss:  1137.9161989361073\n",
            "Step:  936 epoch:  2 training_loss:  1141.8291532665517\n",
            "Step:  937 epoch:  2 training_loss:  1145.647250142696\n",
            "Step:  938 epoch:  2 training_loss:  1149.7173194557454\n",
            "Step:  939 epoch:  2 training_loss:  1153.4015261799123\n",
            "Step:  940 epoch:  2 training_loss:  1157.5104767948415\n",
            "Step:  941 epoch:  2 training_loss:  1161.262410369518\n",
            "Step:  942 epoch:  2 training_loss:  1165.179791179302\n",
            "Step:  943 epoch:  2 training_loss:  1168.8494379192616\n",
            "Step:  944 epoch:  2 training_loss:  1173.0703689724232\n",
            "Step:  945 epoch:  2 training_loss:  1177.04172703326\n",
            "Step:  946 epoch:  2 training_loss:  1180.6724762588765\n",
            "Step:  947 epoch:  2 training_loss:  1184.682677713039\n",
            "Step:  948 epoch:  2 training_loss:  1188.7280158668782\n",
            "Step:  949 epoch:  2 training_loss:  1192.8503684669759\n",
            "Step:  950 epoch:  2 training_loss:  1196.966483560207\n",
            "Step:  951 epoch:  2 training_loss:  1200.981300798061\n",
            "Step:  952 epoch:  2 training_loss:  1204.9718880325581\n",
            "Step:  953 epoch:  2 training_loss:  1208.3771900802876\n",
            "Step:  954 epoch:  2 training_loss:  1212.4333395630147\n",
            "Step:  955 epoch:  2 training_loss:  1216.670603242519\n",
            "Step:  956 epoch:  2 training_loss:  1220.7555493980672\n",
            "Step:  957 epoch:  2 training_loss:  1224.3235699802663\n",
            "Step:  958 epoch:  2 training_loss:  1228.1531312137868\n",
            "Step:  959 epoch:  2 training_loss:  1232.2689816623952\n",
            "Step:  960 epoch:  2 training_loss:  1236.1683792740132\n",
            "Step:  961 epoch:  2 training_loss:  1240.0438394218709\n",
            "Step:  962 epoch:  2 training_loss:  1244.166127172115\n",
            "Step:  963 epoch:  2 training_loss:  1248.0008759170796\n",
            "Step:  964 epoch:  2 training_loss:  1252.1622404724385\n",
            "Step:  965 epoch:  2 training_loss:  1255.9228899151112\n",
            "Step:  966 epoch:  2 training_loss:  1259.8758210808064\n",
            "Step:  967 epoch:  2 training_loss:  1263.7461473614003\n",
            "Step:  968 epoch:  2 training_loss:  1267.727413859966\n",
            "Step:  969 epoch:  2 training_loss:  1271.715206828716\n",
            "Step:  970 epoch:  2 training_loss:  1275.328819957378\n",
            "Step:  971 epoch:  2 training_loss:  1279.3762256771352\n",
            "Step:  972 epoch:  2 training_loss:  1283.2330708175923\n",
            "Step:  973 epoch:  2 training_loss:  1287.2649244934346\n",
            "Step:  974 epoch:  2 training_loss:  1290.8143713146474\n",
            "Step:  975 epoch:  2 training_loss:  1294.9525100857045\n",
            "Step:  976 epoch:  2 training_loss:  1298.9423451095845\n",
            "Step:  977 epoch:  2 training_loss:  1302.8265328079488\n",
            "Step:  978 epoch:  2 training_loss:  1306.5214557319905\n",
            "Step:  979 epoch:  2 training_loss:  1310.413330045345\n",
            "Step:  980 epoch:  2 training_loss:  1314.4971203476216\n",
            "Step:  981 epoch:  2 training_loss:  1318.756097760799\n",
            "Step:  982 epoch:  2 training_loss:  1322.3677870899464\n",
            "Step:  983 epoch:  2 training_loss:  1326.584980693462\n",
            "Step:  984 epoch:  2 training_loss:  1330.5687937408711\n",
            "Epoch: 2 \tTraining Loss: 0.203078 \tValidation Loss: 4.109009\n",
            "Validation loss decreased (4.336639 --> 4.109009).  Saving model ...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}